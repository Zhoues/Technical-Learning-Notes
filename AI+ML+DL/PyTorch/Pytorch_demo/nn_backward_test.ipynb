{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播优化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一转换为 tensor 类型\n",
    "dataset_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_set = torchvision.datasets.CIFAR10(root='./dataset/CIFAR10',train=True, transform=dataset_transform, download=True)\n",
    "# 测试集\n",
    "test_set = torchvision.datasets.CIFAR10(root='./dataset/CIFAR10',train=False, transform=dataset_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建加载数据的对象\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    # 重写构造函数，确定神经网络的构成\n",
    "    def __init__(self):\n",
    "        # 调用父类进行初始化\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3,32,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32,32,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32,64,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024,64),\n",
    "            Linear(64,10)\n",
    "        )\n",
    "    # 重写前向运算\n",
    "    def forward(self, x):\n",
    "       return self.model1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3279, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2935, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3233, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3062, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3036, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3146, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3191, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3118, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2902, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3188, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2916, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3192, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3070, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2987, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3060, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3043, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3106, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2976, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2924, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3095, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3188, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3140, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3161, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3160, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3225, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3153, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2920, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3134, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3125, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2857, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3066, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3026, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3106, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3039, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3079, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3232, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2967, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3274, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3027, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2972, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2900, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2907, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2875, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3203, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3197, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2997, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3020, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3345, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2961, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2962, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3131, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3115, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2846, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3088, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3152, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3022, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3064, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3185, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3177, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3148, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3219, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2988, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3006, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3138, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3152, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3137, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3301, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3155, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2923, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3136, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3216, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3150, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2960, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2999, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3144, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3234, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2971, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3037, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3095, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3111, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3048, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3230, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2941, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3217, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3054, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3153, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2935, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3129, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2971, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2952, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3190, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2818, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2933, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2906, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3235, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3239, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3155, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3103, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2832, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3098, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2954, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3098, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3046, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3159, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2977, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3078, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3042, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3255, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3249, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3018, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3004, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3158, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2973, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2972, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2986, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3195, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3456, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    imgs, targets = data\n",
    "    output = mymodel(imgs)\n",
    "    result_loss = loss(output,targets)\n",
    "    # 进行反向传播优化\n",
    "    result_loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43b0a550ef5c6a967954effa912da879379e1c16afb6534ad7d5212b4a32c414"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
